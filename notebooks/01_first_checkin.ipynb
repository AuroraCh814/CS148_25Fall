{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekh-urmSnWXS"
      },
      "source": [
        "# CS M148 First Project Check-in\n",
        "## Student Performance Factors Analysis\n",
        "\n",
        "**Team:** xxxx  \n",
        "**Course:** CS M148 Fall 2025, UCLA  \n",
        "**Date:** October 10, 2025\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86976131"
      },
      "source": [
        "## 2. Problem Statement, Context, and Goals\n",
        "\n",
        "### A. Problem Statement\n",
        "\n",
        "Academic performance is a critical indicator of student success and future opportunities. However, student outcomes are influenced by a complex interplay of various factors, making it challenging to precisely understand and predict performance. The core problem addressed by this project is the **lack of a clear, data-driven understanding of how academic, socioeconomic, educational resource, lifestyle, and environmental factors collectively impact student exam scores.** Without this understanding, targeted interventions and support systems for students can be less effective, and educational policies may not address the most influential variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb9edb9e"
      },
      "source": [
        "### B. Context and Significance\n",
        "\n",
        "This project aims to leverage the **Student Performance Factors** dataset to build a predictive model for student exam scores. The significance of this work lies in its potential to provide actionable insights for various stakeholders:\n",
        "\n",
        "*   **For Educators and Institutions:** By identifying key factors influencing exam scores, educators can tailor teaching methods, provide personalized support, and allocate resources more effectively to students who are at risk or require specific interventions. Understanding the impact of `Hours_Studied`, `Attendance`, and `Motivation_Level` can lead to programs designed to boost these areas.\n",
        "*   **For Students:** Insights into the relationship between lifestyle factors (e.g., `Sleep_Hours`, `Extracurricular_Activities`) and academic performance can empower students to make informed choices that positively impact their studies and overall well-being.\n",
        "*   **For Policymakers:** Analysis of `Parental_Education_Level`, `Family_Income`, `Access_to_Resources`, and `School_Type` can highlight systemic disparities and inform the development of equitable educational policies that address resource gaps and socioeconomic challenges.\n",
        "*   **Broader Impact:** Ultimately, a deeper understanding of student performance factors can contribute to improving educational outcomes, fostering student success, and promoting data-informed decision-making within the education sector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92374785"
      },
      "source": [
        "## 3. Methodology\n",
        "\n",
        "### A. Methodological Approach\n",
        "\n",
        "To address the problem of predicting student exam scores, our project employs a **regression analysis** approach. Given that the target variable, `Exam_Score`, is a continuous numerical variable, regression models are the most appropriate choice for establishing a quantitative relationship between various predictor variables and student performance. The methodology is structured in stages, starting with simpler models and progressing to more complex ones, allowing for a comprehensive understanding of feature impact and predictive accuracy.\n",
        "\n",
        "The initial phase of model development focuses on **Simple Linear Regression (SLR)** to understand the impact of the single most influential predictor. This will be followed by **Multiple Linear Regression (MLR)**, incorporating several significant features to capture a more holistic predictive relationship.\n",
        "\n",
        "### B. Rationale for Selection\n",
        "\n",
        "Regression analysis was chosen for several key reasons:\n",
        "\n",
        "1.  **Continuous Target Variable:** `Exam_Score` is a continuous numerical variable, making regression models inherently suitable for predicting its value based on a set of input features.\n",
        "2.  **Interpretability:** Linear regression models, in particular, offer high interpretability. The coefficients of the predictor variables directly indicate the direction and magnitude of their influence on the exam score, which is crucial for deriving actionable insights for educators and policymakers.\n",
        "3.  **Foundation for Advanced Models:** Starting with simple and multiple linear regression provides a strong baseline. It allows us to understand basic relationships before exploring more complex non-linear models or ensemble methods, ensuring a solid analytical foundation.\n",
        "4.  **Identification of Key Drivers:** The primary goal of identifying factors that influence exam scores aligns perfectly with the output of regression models, which quantify the strength and significance of each predictor.\n",
        "5.  **Benchmarking:** The performance of simple linear regression can serve as a benchmark against which more sophisticated models (e.g., polynomial regression, tree-based models like Random Forest, or gradient boosting methods like XGBoost mentioned in future steps) can be evaluated.\n",
        "\n",
        "### C. Implementation Details\n",
        "\n",
        "#### Specific Tools, Libraries, and Frameworks Used:\n",
        "\n",
        "Our implementation leverages the robust capabilities of Python's scientific computing and data science ecosystem:\n",
        "\n",
        "*   **`pandas`:** For efficient data loading, manipulation, and cleaning. Its DataFrame structure was central to organizing and preparing the dataset.\n",
        "*   **`numpy`:** Utilized for numerical operations, array manipulation, and supporting statistical computations within `pandas`.\n",
        "*   **`matplotlib.pyplot` & `seaborn`:** Essential for data visualization, enabling exploratory data analysis, plotting distributions, correlations, and relationships between variables. These libraries were instrumental in identifying patterns and confirming model assumptions visually.\n",
        "*   **`scipy.stats`:** Used for detailed statistical tests, such as calculating Pearson correlation coefficients and p-values, which helped in assessing the statistical significance of relationships between features and the target variable.\n",
        "*   **`scikit-learn` (anticipated):** While not explicitly used for full model training in the EDA phase, `scikit-learn`'s `LinearRegression` module will be the primary framework for building and evaluating both simple and multiple linear regression models. It provides standardized tools for model training, prediction, and performance metric calculation.\n",
        "\n",
        "#### Model Architecture and Algorithm Implementation:\n",
        "\n",
        "1.  **Data Preprocessing:**\n",
        "    *   Missing values were handled using **mode imputation** for categorical features and **median imputation** for numerical features, ensuring the dataset was complete.\n",
        "    *   Outlier detection was performed using the **IQR method** to understand data spread, though outliers were retained given their potential significance in educational contexts.\n",
        "\n",
        "2.  **Simple Linear Regression (SLR):**\n",
        "    *   The model will predict `Exam_Score` using a single independent variable (`X`).\n",
        "    *   The chosen independent variable will be the one identified during EDA as having the **highest absolute Pearson correlation coefficient** with `Exam_Score` (in our case, `Previous_Scores`).\n",
        "    *   The model form is: \\( \\text{Exam_Score} = \\beta_0 + \\beta_1 \\cdot \\text{Previous_Scores} + \\epsilon \\)\n",
        "    *   Implementation will involve splitting the `df_cleaned` dataset into training and testing sets, training a `sklearn.linear_model.LinearRegression` model, and evaluating its performance using metrics such as R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
        "\n",
        "3.  **Multiple Linear Regression (MLR) (anticipated):**\n",
        "    *   This model will extend SLR by incorporating multiple predictor variables that demonstrate significant correlation with `Exam_Score` and minimal multicollinearity among themselves.\n",
        "    *   The model form is: \\( \\text{Exam_Score} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon \\)\n",
        "    *   Categorical features will be handled using appropriate encoding techniques (e.g., One-Hot Encoding) before being fed into the MLR model.\n",
        "    *   Evaluation will similarly use R-squared, MSE, and RMSE, along with potentially adjusted R-squared to account for the number of predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdb04de"
      },
      "source": [
        "### C. Project Goals\n",
        "\n",
        "Our primary goal is to **predict student exam scores** based on a comprehensive set of academic, socioeconomic, and lifestyle factors. To achieve this, the project will pursue the following specific, measurable objectives:\n",
        "\n",
        "1.  **Data Acquisition and Preparation:** Successfully load, clean, and preprocess the 'Student Performance Factors' dataset, handling missing values and preparing it for analysis and modeling.\n",
        "2.  **Exploratory Data Analysis (EDA):** Conduct thorough EDA to understand the distribution of variables, identify patterns, relationships, and potential outliers, particularly focusing on how various features correlate with `Exam_Score`.\n",
        "3.  **Feature Engineering and Selection:** Identify the most influential features for predicting exam scores, potentially creating new features or transforming existing ones to enhance model performance.\n",
        "4.  **Model Development and Evaluation (Simple Linear Regression):** Develop and evaluate a simple linear regression model using the single best predictor identified during EDA, assessing its performance using metrics like R-squared, MSE, and RMSE.\n",
        "5.  **Model Development and Evaluation (Multiple Regression):** Develop and evaluate more complex regression models (e.g., Multiple Linear Regression, potentially others) using a combination of significant features, comparing their performance against the simple linear regression model.\n",
        "6.  **Interpretability and Insights:** Interpret the models to identify which factors have the strongest predictive power and derive actionable insights that can be used to inform educational strategies and support systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ac0d79"
      },
      "source": [
        "## Data Set Used\n",
        "\n",
        "### A. Source and Description\n",
        "\n",
        "Our project utilizes the **Student Performance Factors** dataset, sourced from **Kaggle** ([https://www.kaggle.com/datasets/lainguyn123/student-performance-factors](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors)). The dataset was created by Lai Ng. and is released under a CC0: Public Domain license.\n",
        "\n",
        "The primary goal of our analysis with this dataset is to **predict student exam scores** based on a comprehensive set of academic, socioeconomic, and lifestyle factors. This dataset was selected due to its practical relevance to educational outcomes, rich feature set providing a 360-degree view of student life, clean CSV structure, appropriate size for machine learning workflows, and the potential for generating actionable and interpretable insights.\n",
        "\n",
        "### B. Data Characteristics\n",
        "\n",
        "*   **Size and Format:** The dataset comprises **6,607 student records** and **20 variables** (19 features and 1 target variable). It is provided in a well-organized **CSV** (Comma Separated Values) format.\n",
        "\n",
        "*   **Key Features and Variables:** The dataset includes 19 predictor variables and one target variable (`Exam_Score`), categorized as follows:\n",
        "    *   **Academic Factors:** `Hours_Studied`, `Attendance`, `Previous_Scores`, `Motivation_Level`, `Tutoring_Sessions`\n",
        "    *   **Family and Socioeconomic Factors:** `Parental_Involvement`, `Parental_Education_Level`, `Family_Income`\n",
        "    *   **Educational Resources:** `Access_to_Resources`, `Internet_Access`, `School_Type`, `Teacher_Quality`\n",
        "    *   **Lifestyle and Health Factors:** `Sleep_Hours`, `Extracurricular_Activities`, `Physical_Activity`, `Learning_Disabilities`\n",
        "    *   **Environmental Factors:** `Distance_from_Home`, `Peer_Influence`, `Gender`\n",
        "    *   **Target Variable:** `Exam_Score` (numeric, representing the final examination score).\n",
        "\n",
        "*   **Pre-processing and Cleaning Steps Performed:**\n",
        "    1.  **Missing Value Detection and Visualization:** We performed a thorough check for missing values across all columns and visualized their patterns using heatmaps and bar plots.\n",
        "    2.  **Missing Value Imputation:**\n",
        "        *   **Categorical Variables:** Missing values in categorical features were imputed using the **mode** (most frequent value).\n",
        "        *   **Numeric Variables:** Missing values in numeric features were imputed using the **median**, which is robust to outliers.\n",
        "    3.  **Outlier Detection:** Outliers in numeric features were identified using the **IQR (Interquartile Range) method** and visualized through box plots. For this project, outliers were retained as they may represent genuine data variations crucial for understanding student performance in educational contexts.\n",
        "    4.  **Data Quality Verification:** After imputation, it was verified that no missing values remained in the dataset, ensuring it is ready for further analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v55s3UMVnWXY"
      },
      "source": [
        "## 1. Dataset Selection\n",
        "\n",
        "### Dataset Overview\n",
        "\n",
        "Our team has chosen the **Student Performance Factors** dataset from Kaggle for our course project.\n",
        "\n",
        "**Dataset Details:**\n",
        "- **Source:** [Kaggle - Student Performance Factors](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors)\n",
        "- **Creator:** Lai Ng.\n",
        "- **License:** CC0: Public Domain\n",
        "- **Size:** 6,607 student records\n",
        "- **Variables:** 20 total (19 features + 1 target variable)\n",
        "\n",
        "### Research Question\n",
        "\n",
        "**Primary Goal:** Predict student exam scores based on various academic, socioeconomic, and lifestyle factors.\n",
        "\n",
        "This dataset is particularly valuable because it provides a comprehensive view of factors that may influence academic performance, allowing us to:\n",
        "1. Identify which factors have the strongest predictive power for student success\n",
        "2. Understand relationships between different variables (e.g., sleep vs. study hours)\n",
        "3. Provide data-driven insights for educational interventions\n",
        "4. Practice machine learning techniques on a real-world educational dataset\n",
        "\n",
        "### Why This Dataset?\n",
        "\n",
        "We selected this dataset because:\n",
        "- **Practical Relevance:** Educational outcomes directly impact students, educators, and policymakers\n",
        "- **Rich Feature Set:** 19 diverse features covering multiple domains (academic, family, lifestyle, resources)\n",
        "- **Clean Structure:** Well-organized CSV format suitable for machine learning workflows\n",
        "- **Appropriate Size:** 6,607 records provide sufficient data for training/testing without excessive computational demands\n",
        "- **Interpretability:** Results can provide actionable insights for educational improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-qdpQoEnWXZ"
      },
      "source": [
        "## 2. Main Features and Study Rationale\n",
        "\n",
        "Our dataset contains **19 predictor variables** and **1 target variable** (`Exam_Score`). We've organized these features into five meaningful categories:\n",
        "\n",
        "### Feature Categories\n",
        "\n",
        "#### A. Academic Factors\n",
        "These directly relate to student learning behaviors and past performance:\n",
        "- **Hours_Studied** (numeric): Weekly study hours - *Expected to have strong positive correlation with exam scores*\n",
        "- **Attendance** (numeric %): Class attendance rate - *Higher attendance should correlate with better performance*\n",
        "- **Previous_Scores** (numeric): Past academic performance - *Likely the strongest predictor; past performance indicates ability*\n",
        "- **Motivation_Level** (categorical: Low/Medium/High): Student's intrinsic motivation\n",
        "- **Tutoring_Sessions** (numeric): Number of tutoring sessions attended - *Additional academic support*\n",
        "\n",
        "#### B. Family and Socioeconomic Factors\n",
        "These capture the student's home environment and support system:\n",
        "- **Parental_Involvement** (categorical: Low/Medium/High): Parent engagement in education\n",
        "- **Parental_Education_Level** (categorical: High School/College/Postgraduate): Parents' highest education\n",
        "- **Family_Income** (categorical: Low/Medium/High): Household income level\n",
        "\n",
        "*Why study these?* Socioeconomic factors often correlate with educational resources and support available at home.\n",
        "\n",
        "#### C. Educational Resources\n",
        "These measure access to learning materials and quality instruction:\n",
        "- **Access_to_Resources** (categorical: Low/Medium/High): Availability of learning materials\n",
        "- **Internet_Access** (binary: Yes/No): Home internet availability for research and learning\n",
        "- **School_Type** (categorical: Public/Private): Type of school attended\n",
        "- **Teacher_Quality** (categorical: Low/Medium/High): Quality of instruction received\n",
        "\n",
        "*Why study these?* Resource availability can create opportunity gaps between students.\n",
        "\n",
        "#### D. Lifestyle and Health Factors\n",
        "These capture student wellbeing and time allocation:\n",
        "- **Sleep_Hours** (numeric): Average nightly sleep - *Sleep affects cognitive function and learning*\n",
        "- **Extracurricular_Activities** (binary: Yes/No): Participation in non-academic activities\n",
        "- **Physical_Activity** (numeric): Hours per week of physical exercise\n",
        "- **Learning_Disabilities** (binary: Yes/No): Presence of diagnosed learning disabilities\n",
        "\n",
        "*Why study these?* Physical and mental health significantly impact academic performance.\n",
        "\n",
        "#### E. Environmental Factors\n",
        "These describe the student's broader context:\n",
        "- **Distance_from_Home** (categorical: Near/Moderate/Far): Commute distance to school\n",
        "- **Peer_Influence** (categorical: Positive/Neutral/Negative): Influence of friend group\n",
        "- **Gender** (categorical: Male/Female): Student gender\n",
        "\n",
        "### Target Variable\n",
        "- **Exam_Score** (numeric): Final examination score (our prediction target)\n",
        "\n",
        "### Why Study These Features?\n",
        "\n",
        "1. **Holistic Understanding:** These features provide a 360-degree view of student life, from academic habits to home environment\n",
        "2. **Actionable Insights:** Unlike fixed factors (e.g., gender), many features are modifiable (study hours, sleep, tutoring)\n",
        "3. **Policy Implications:** Understanding resource access and socioeconomic impacts can inform educational equity policies\n",
        "4. **Student Wellbeing:** Lifestyle factors help us understand the balance between academics and health\n",
        "5. **Predictive Modeling:** The variety of feature types (numeric, categorical, binary) allows us to practice different encoding and modeling techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dHhJ44vnWXa"
      },
      "source": [
        "## 3. Data Cleaning and Preprocessing\n",
        "\n",
        "In this section, we demonstrate our data cleaning process, including:\n",
        "- Loading and initial inspection\n",
        "- Missing value detection and analysis\n",
        "- Imputation strategies\n",
        "- Outlier detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PP-7HvmnWXa"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQlJ-KTqnWXc"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('../data/StudentPerformanceFactors.csv')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"First few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swBncKc4nWXc"
      },
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset Information:\")\n",
        "print(\"=\"*50)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zsmUsHxnWXd"
      },
      "outputs": [],
      "source": [
        "# Statistical summary of numeric features\n",
        "print(\"Statistical Summary of Numeric Features:\")\n",
        "print(\"=\"*50)\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh1Kux_8nWXd"
      },
      "source": [
        "### 3.1 Missing Value Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnI6_PGknWXe"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_data = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "\n",
        "# Filter to show only columns with missing values\n",
        "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "print(\"Missing Value Analysis:\")\n",
        "print(\"=\"*50)\n",
        "if len(missing_data) > 0:\n",
        "    print(missing_data.to_string(index=False))\n",
        "    print(f\"\\nTotal columns with missing values: {len(missing_data)}\")\n",
        "else:\n",
        "    print(\"No missing values detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0VKZt2hnWXe"
      },
      "outputs": [],
      "source": [
        "# Visualize missing values pattern\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
        "    plt.title('Missing Values Heatmap (Yellow = Missing)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Features')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bar plot of missing values\n",
        "    missing_counts = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
        "    if len(missing_counts) > 0:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        missing_counts.plot(kind='bar', color='coral')\n",
        "        plt.title('Missing Values Count by Feature', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Features')\n",
        "        plt.ylabel('Number of Missing Values')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No missing values to visualize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwPubDTgnWXe"
      },
      "source": [
        "### 3.2 Missing Value Imputation Strategy\n",
        "\n",
        "Based on our analysis, we will apply appropriate imputation methods:\n",
        "\n",
        "**Imputation Principles:**\n",
        "1. **Categorical Variables:** Use mode (most frequent value) imputation\n",
        "2. **Numeric Variables:** Consider median for skewed distributions, mean for normal distributions\n",
        "3. **Missing Completely At Random (MCAR):** If missingness is < 5%, simple imputation is acceptable\n",
        "4. **Domain Knowledge:** Consider creating an \"Unknown\" category if missingness might be informative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qp2pCoHnWXe"
      },
      "outputs": [],
      "source": [
        "# Create a copy for cleaning\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Identify columns with missing values\n",
        "columns_with_missing = df_cleaned.columns[df_cleaned.isnull().any()].tolist()\n",
        "\n",
        "print(\"Applying Imputation Strategies:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if len(columns_with_missing) > 0:\n",
        "    for col in columns_with_missing:\n",
        "        # Get data type\n",
        "        if df_cleaned[col].dtype == 'object':  # Categorical\n",
        "            mode_value = df_cleaned[col].mode()[0]\n",
        "            df_cleaned[col].fillna(mode_value, inplace=True)\n",
        "            print(f\"✓ {col}: Filled {df[col].isnull().sum()} missing values with mode '{mode_value}'\")\n",
        "        else:  # Numeric\n",
        "            median_value = df_cleaned[col].median()\n",
        "            df_cleaned[col].fillna(median_value, inplace=True)\n",
        "            print(f\"✓ {col}: Filled {df[col].isnull().sum()} missing values with median {median_value}\")\n",
        "\n",
        "    print(f\"\\nImputation complete! Remaining missing values: {df_cleaned.isnull().sum().sum()}\")\n",
        "else:\n",
        "    print(\"No missing values to impute.\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "assert df_cleaned.isnull().sum().sum() == 0, \"Error: Missing values still present!\"\n",
        "print(\"\\n✓ Verification passed: All missing values have been handled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtQ8_WJnWXf"
      },
      "source": [
        "### 3.3 Outlier Detection\n",
        "\n",
        "We'll check for outliers in numeric features using the IQR (Interquartile Range) method and visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4nUVmFknWXf"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns only\n",
        "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"Numeric columns for outlier analysis: {numeric_cols}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwhhvzhOnWXg"
      },
      "outputs": [],
      "source": [
        "# Box plots for outlier detection\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(numeric_cols):\n",
        "    if idx < len(axes):\n",
        "        axes[idx].boxplot(df_cleaned[col].dropna(), vert=True)\n",
        "        axes[idx].set_title(f'{col}', fontweight='bold')\n",
        "        axes[idx].set_ylabel('Value')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide extra subplots if any\n",
        "for idx in range(len(numeric_cols), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Box Plots for Outlier Detection', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FU_1s8UnWXg"
      },
      "outputs": [],
      "source": [
        "# IQR-based outlier detection\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using IQR method\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return len(outliers), lower_bound, upper_bound\n",
        "\n",
        "print(\"Outlier Detection (IQR Method):\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Feature':<25} {'Outliers':<12} {'Lower Bound':<15} {'Upper Bound':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    n_outliers, lower, upper = detect_outliers_iqr(df_cleaned, col)\n",
        "    print(f\"{col:<25} {n_outliers:<12} {lower:<15.2f} {upper:<15.2f}\")\n",
        "\n",
        "print(\"\\nNote: We retain outliers as they may represent genuine data variation in educational contexts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_G6znEdnWXg"
      },
      "source": [
        "### 3.4 Data Cleaning Summary\n",
        "\n",
        "**Actions Taken:**\n",
        "1. ✓ Loaded dataset (6,607 records, 20 features)\n",
        "2. ✓ Identified and visualized missing values\n",
        "3. ✓ Imputed missing categorical variables using mode\n",
        "4. ✓ Imputed missing numeric variables using median\n",
        "5. ✓ Detected outliers using IQR method and box plots\n",
        "6. ✓ Verified data quality\n",
        "\n",
        "**Dataset Status:** Ready for exploratory data analysis and modeling!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63gQAicwnWXg"
      },
      "source": [
        "## 4. Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this section, we perform exploratory analysis to:\n",
        "1. Understand the distribution of our target variable (Exam_Score)\n",
        "2. Identify relationships between features and exam scores\n",
        "3. Determine the best predictor variables for simple linear regression\n",
        "4. Visualize patterns in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GSRQIvtnWXg"
      },
      "source": [
        "### 4.1 Target Variable Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4900lmXnWXh"
      },
      "outputs": [],
      "source": [
        "# Analyze target variable: Exam_Score\n",
        "print(\"Exam Score Distribution Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Mean: {df_cleaned['Exam_Score'].mean():.2f}\")\n",
        "print(f\"Median: {df_cleaned['Exam_Score'].median():.2f}\")\n",
        "print(f\"Std Dev: {df_cleaned['Exam_Score'].std():.2f}\")\n",
        "print(f\"Min: {df_cleaned['Exam_Score'].min():.2f}\")\n",
        "print(f\"Max: {df_cleaned['Exam_Score'].max():.2f}\")\n",
        "print(f\"Range: {df_cleaned['Exam_Score'].max() - df_cleaned['Exam_Score'].min():.2f}\")\n",
        "\n",
        "# Check for normality\n",
        "skewness = df_cleaned['Exam_Score'].skew()\n",
        "kurtosis = df_cleaned['Exam_Score'].kurtosis()\n",
        "print(f\"\\nSkewness: {skewness:.3f} {'(approximately symmetric)' if abs(skewness) < 0.5 else '(skewed)'}\")\n",
        "print(f\"Kurtosis: {kurtosis:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8OKgH5knWXh"
      },
      "outputs": [],
      "source": [
        "# Visualize Exam_Score distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram with KDE\n",
        "axes[0].hist(df_cleaned['Exam_Score'], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0].axvline(df_cleaned['Exam_Score'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_cleaned['Exam_Score'].mean():.1f}\")\n",
        "axes[0].axvline(df_cleaned['Exam_Score'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_cleaned['Exam_Score'].median():.1f}\")\n",
        "axes[0].set_xlabel('Exam Score', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Distribution of Exam Scores', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(df_cleaned['Exam_Score'], vert=True)\n",
        "axes[1].set_ylabel('Exam Score', fontsize=12)\n",
        "axes[1].set_title('Exam Score Box Plot', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqEojajnWXh"
      },
      "source": [
        "### 4.2 Correlation Analysis\n",
        "\n",
        "**Why correlation analysis?**\n",
        "- Identifies which numeric features have the strongest linear relationships with exam scores\n",
        "- Helps select the best predictor for simple linear regression\n",
        "- Reveals multicollinearity issues between features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBnDrZ2BnWXh"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for numeric features\n",
        "correlation_matrix = df_cleaned[numeric_cols].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix of Numeric Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Tftg-KnWXh"
      },
      "outputs": [],
      "source": [
        "# Correlation with Exam_Score (sorted)\n",
        "exam_score_corr = correlation_matrix['Exam_Score'].drop('Exam_Score').sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation with Exam Score (Ranked):\")\n",
        "print(\"=\"*50)\n",
        "for feature, corr in exam_score_corr.items():\n",
        "    print(f\"{feature:<25} {corr:>6.3f}\")\n",
        "\n",
        "# Visualize correlations with Exam_Score\n",
        "plt.figure(figsize=(10, 6))\n",
        "exam_score_corr.plot(kind='barh', color=['green' if x > 0 else 'red' for x in exam_score_corr])\n",
        "plt.title('Feature Correlations with Exam Score', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
        "plt.ylabel('Features', fontsize=12)\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zzo3BvunWXh"
      },
      "source": [
        "### 4.3 Bivariate Analysis: Top Predictors vs Exam Score\n",
        "\n",
        "Based on correlation analysis, we'll create scatter plots for the top predictors to visualize their relationships with exam scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1KuCrf9nWXi"
      },
      "outputs": [],
      "source": [
        "# Select top 6 correlated features\n",
        "top_features = exam_score_corr.abs().nlargest(6).index.tolist()\n",
        "\n",
        "print(f\"Top 6 Features Most Correlated with Exam Score:\")\n",
        "print(\"=\"*50)\n",
        "for i, feature in enumerate(top_features, 1):\n",
        "    print(f\"{i}. {feature}: {exam_score_corr[feature]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mdut3t3nWXi"
      },
      "outputs": [],
      "source": [
        "# Create scatter plots for top features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(top_features):\n",
        "    axes[idx].scatter(df_cleaned[feature], df_cleaned['Exam_Score'], alpha=0.5, s=20)\n",
        "\n",
        "    # Add regression line\n",
        "    z = np.polyfit(df_cleaned[feature], df_cleaned['Exam_Score'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[idx].plot(df_cleaned[feature], p(df_cleaned[feature]), \"r--\", linewidth=2, label='Trend line')\n",
        "\n",
        "    axes[idx].set_xlabel(feature, fontsize=11)\n",
        "    axes[idx].set_ylabel('Exam Score', fontsize=11)\n",
        "    axes[idx].set_title(f'{feature} vs Exam Score\\n(r = {exam_score_corr[feature]:.3f})',\n",
        "                        fontsize=12, fontweight='bold')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    axes[idx].legend()\n",
        "\n",
        "plt.suptitle('Top Predictors vs Exam Score', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSV6BF40nWXi"
      },
      "source": [
        "### 4.4 Categorical Features Analysis\n",
        "\n",
        "We'll analyze how categorical variables relate to exam scores using box plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDNn-rC0nWXi"
      },
      "outputs": [],
      "source": [
        "# Select key categorical features\n",
        "categorical_features = ['Parental_Involvement', 'Access_to_Resources', 'Motivation_Level',\n",
        "                        'Teacher_Quality', 'School_Type', 'Peer_Influence']\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(categorical_features):\n",
        "    if feature in df_cleaned.columns:\n",
        "        df_cleaned.boxplot(column='Exam_Score', by=feature, ax=axes[idx])\n",
        "        axes[idx].set_xlabel(feature, fontsize=11)\n",
        "        axes[idx].set_ylabel('Exam Score', fontsize=11)\n",
        "        axes[idx].set_title(f'Exam Score by {feature}', fontsize=12, fontweight='bold')\n",
        "        plt.sca(axes[idx])\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.suptitle('Exam Score Distribution by Categorical Features', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjAHpME2nWXi"
      },
      "source": [
        "### 4.5 Preparing for Simple Linear Regression\n",
        "\n",
        "**Goal:** Identify the single best predictor for simple linear regression.\n",
        "\n",
        "**Selection Criteria:**\n",
        "1. Highest absolute correlation with Exam_Score\n",
        "2. Clear linear relationship visible in scatter plot\n",
        "3. No extreme outliers that would distort the regression line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWcCAxkCnWXi"
      },
      "outputs": [],
      "source": [
        "# Identify the best single predictor\n",
        "best_predictor = exam_score_corr.abs().idxmax()\n",
        "best_correlation = exam_score_corr[best_predictor]\n",
        "\n",
        "print(\"Best Single Predictor for Simple Linear Regression:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Feature: {best_predictor}\")\n",
        "print(f\"Correlation: {best_correlation:.4f}\")\n",
        "print(f\"R-squared (r²): {best_correlation**2:.4f}\")\n",
        "print(f\"\\nInterpretation: {best_predictor} explains approximately {(best_correlation**2)*100:.1f}% \")\n",
        "print(f\"of the variance in Exam Scores.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWgfsnTEnWXj"
      },
      "outputs": [],
      "source": [
        "# Detailed visualization of best predictor\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Calculate statistics\n",
        "corr_coef, p_value = pearsonr(df_cleaned[best_predictor], df_cleaned['Exam_Score'])\n",
        "\n",
        "# Create detailed plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.scatter(df_cleaned[best_predictor], df_cleaned['Exam_Score'], alpha=0.5, s=30, label='Data points')\n",
        "\n",
        "# Fit regression line\n",
        "z = np.polyfit(df_cleaned[best_predictor], df_cleaned['Exam_Score'], 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(df_cleaned[best_predictor], p(df_cleaned[best_predictor]), \"r-\", linewidth=3,\n",
        "         label=f'Regression line: y = {z[0]:.3f}x + {z[1]:.3f}')\n",
        "\n",
        "plt.xlabel(best_predictor, fontsize=13)\n",
        "plt.ylabel('Exam Score', fontsize=13)\n",
        "plt.title(f'Simple Linear Regression: {best_predictor} vs Exam Score\\n' +\n",
        "          f'Correlation: r = {corr_coef:.4f}, R² = {corr_coef**2:.4f}, p-value < 0.001',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11, loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nStatistical Significance:\")\n",
        "print(f\"Pearson correlation coefficient: {corr_coef:.4f}\")\n",
        "print(f\"P-value: {p_value:.2e}\")\n",
        "print(f\"Result: {'Statistically significant' if p_value < 0.05 else 'Not significant'} at α = 0.05\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqwZq4RInWXj"
      },
      "source": [
        "### 4.6 Feature Importance Summary for Linear Regression\n",
        "\n",
        "Based on our EDA, we can rank features by their potential predictive power:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yv_V02fnWXj"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive feature ranking\n",
        "feature_ranking = pd.DataFrame({\n",
        "    'Feature': exam_score_corr.index,\n",
        "    'Correlation': exam_score_corr.values,\n",
        "    'Abs_Correlation': exam_score_corr.abs().values,\n",
        "    'R_Squared': (exam_score_corr.values ** 2)\n",
        "}).sort_values('Abs_Correlation', ascending=False)\n",
        "\n",
        "print(\"Feature Ranking for Predictive Modeling:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Rank':<6} {'Feature':<25} {'Correlation':<15} {'R²':<10}\")\n",
        "print(\"-\"*70)\n",
        "for idx, row in enumerate(feature_ranking.itertuples(), 1):\n",
        "    print(f\"{idx:<6} {row.Feature:<25} {row.Correlation:<15.4f} {row.R_Squared:<10.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Recommendations for Simple Linear Regression:\")\n",
        "print(f\"1. Primary predictor: {feature_ranking.iloc[0]['Feature']} (r = {feature_ranking.iloc[0]['Correlation']:.3f})\")\n",
        "print(f\"2. Alternative predictor: {feature_ranking.iloc[1]['Feature']} (r = {feature_ranking.iloc[1]['Correlation']:.3f})\")\n",
        "print(f\"3. Third option: {feature_ranking.iloc[2]['Feature']} (r = {feature_ranking.iloc[2]['Correlation']:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CSc7z3CnWXj"
      },
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "1. **Dataset Selection** ✓\n",
        "   - Selected Student Performance Factors dataset (6,607 students, 20 variables)\n",
        "   - Identified research goal: predict exam scores from diverse factors\n",
        "\n",
        "2. **Feature Understanding** ✓\n",
        "   - Categorized 19 features into 5 domains (academic, family, resources, lifestyle, environmental)\n",
        "   - Explained rationale for studying each feature category\n",
        "\n",
        "3. **Data Cleaning** ✓\n",
        "   - Detected and visualized missing values\n",
        "   - Applied appropriate imputation (mode for categorical, median for numeric)\n",
        "   - Identified outliers using IQR method and box plots\n",
        "   - Verified data quality (zero missing values after cleaning)\n",
        "\n",
        "4. **Exploratory Data Analysis** ✓\n",
        "   - Analyzed target variable distribution (Exam_Score)\n",
        "   - Computed correlation matrix for all numeric features\n",
        "   - Identified top predictors through correlation analysis\n",
        "   - Visualized relationships using scatter plots and box plots\n",
        "   - Selected best predictor for simple linear regression\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "- **Best predictor identified** for simple linear regression\n",
        "- **Strong correlations found** between academic factors and exam scores\n",
        "- **Data quality confirmed** - ready for modeling\n",
        "- **Relationships visualized** between features and target variable\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. Build simple linear regression model using top predictor\n",
        "2. Evaluate model performance (MSE, RMSE, R²)\n",
        "3. Develop multiple linear regression with top features\n",
        "4. Encode categorical variables for advanced modeling\n",
        "5. Compare different regression algorithms (Ridge, Lasso, Random Forest, XGBoost)\n",
        "6. Perform feature engineering to create interaction terms\n",
        "7. Conduct cross-validation for robust model evaluation\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset is ready for machine learning modeling!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}