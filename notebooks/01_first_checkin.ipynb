{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS M148 First Project Check-in\n",
    "## Student Performance Factors Analysis\n",
    "\n",
    "**Team:** xxxx  \n",
    "**Course:** CS M148 Fall 2025, UCLA  \n",
    "**Date:** October 10, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Selection\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "Our team has chosen the **Student Performance Factors** dataset from Kaggle for our course project.\n",
    "\n",
    "**Dataset Details:**\n",
    "- **Source:** [Kaggle - Student Performance Factors](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors)\n",
    "- **Creator:** Lai Ng.\n",
    "- **License:** CC0: Public Domain\n",
    "- **Size:** 6,607 student records\n",
    "- **Variables:** 20 total (19 features + 1 target variable)\n",
    "\n",
    "### Research Question\n",
    "\n",
    "**Primary Goal:** Predict student exam scores based on various academic, socioeconomic, and lifestyle factors.\n",
    "\n",
    "This dataset is particularly valuable because it provides a comprehensive view of factors that may influence academic performance, allowing us to:\n",
    "1. Identify which factors have the strongest predictive power for student success\n",
    "2. Understand relationships between different variables (e.g., sleep vs. study hours)\n",
    "3. Provide data-driven insights for educational interventions\n",
    "4. Practice machine learning techniques on a real-world educational dataset\n",
    "\n",
    "### Why This Dataset?\n",
    "\n",
    "We selected this dataset because:\n",
    "- **Practical Relevance:** Educational outcomes directly impact students, educators, and policymakers\n",
    "- **Rich Feature Set:** 19 diverse features covering multiple domains (academic, family, lifestyle, resources)\n",
    "- **Clean Structure:** Well-organized CSV format suitable for machine learning workflows\n",
    "- **Appropriate Size:** 6,607 records provide sufficient data for training/testing without excessive computational demands\n",
    "- **Interpretability:** Results can provide actionable insights for educational improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Features and Study Rationale\n",
    "\n",
    "Our dataset contains **19 predictor variables** and **1 target variable** (`Exam_Score`). We've organized these features into five meaningful categories:\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "#### A. Academic Factors\n",
    "These directly relate to student learning behaviors and past performance:\n",
    "- **Hours_Studied** (numeric): Weekly study hours - *Expected to have strong positive correlation with exam scores*\n",
    "- **Attendance** (numeric %): Class attendance rate - *Higher attendance should correlate with better performance*\n",
    "- **Previous_Scores** (numeric): Past academic performance - *Likely the strongest predictor; past performance indicates ability*\n",
    "- **Motivation_Level** (categorical: Low/Medium/High): Student's intrinsic motivation\n",
    "- **Tutoring_Sessions** (numeric): Number of tutoring sessions attended - *Additional academic support*\n",
    "\n",
    "#### B. Family and Socioeconomic Factors\n",
    "These capture the student's home environment and support system:\n",
    "- **Parental_Involvement** (categorical: Low/Medium/High): Parent engagement in education\n",
    "- **Parental_Education_Level** (categorical: High School/College/Postgraduate): Parents' highest education\n",
    "- **Family_Income** (categorical: Low/Medium/High): Household income level\n",
    "\n",
    "*Why study these?* Socioeconomic factors often correlate with educational resources and support available at home.\n",
    "\n",
    "#### C. Educational Resources\n",
    "These measure access to learning materials and quality instruction:\n",
    "- **Access_to_Resources** (categorical: Low/Medium/High): Availability of learning materials\n",
    "- **Internet_Access** (binary: Yes/No): Home internet availability for research and learning\n",
    "- **School_Type** (categorical: Public/Private): Type of school attended\n",
    "- **Teacher_Quality** (categorical: Low/Medium/High): Quality of instruction received\n",
    "\n",
    "*Why study these?* Resource availability can create opportunity gaps between students.\n",
    "\n",
    "#### D. Lifestyle and Health Factors\n",
    "These capture student wellbeing and time allocation:\n",
    "- **Sleep_Hours** (numeric): Average nightly sleep - *Sleep affects cognitive function and learning*\n",
    "- **Extracurricular_Activities** (binary: Yes/No): Participation in non-academic activities\n",
    "- **Physical_Activity** (numeric): Hours per week of physical exercise\n",
    "- **Learning_Disabilities** (binary: Yes/No): Presence of diagnosed learning disabilities\n",
    "\n",
    "*Why study these?* Physical and mental health significantly impact academic performance.\n",
    "\n",
    "#### E. Environmental Factors\n",
    "These describe the student's broader context:\n",
    "- **Distance_from_Home** (categorical: Near/Moderate/Far): Commute distance to school\n",
    "- **Peer_Influence** (categorical: Positive/Neutral/Negative): Influence of friend group\n",
    "- **Gender** (categorical: Male/Female): Student gender\n",
    "\n",
    "### Target Variable\n",
    "- **Exam_Score** (numeric): Final examination score (our prediction target)\n",
    "\n",
    "### Why Study These Features?\n",
    "\n",
    "1. **Holistic Understanding:** These features provide a 360-degree view of student life, from academic habits to home environment\n",
    "2. **Actionable Insights:** Unlike fixed factors (e.g., gender), many features are modifiable (study hours, sleep, tutoring)\n",
    "3. **Policy Implications:** Understanding resource access and socioeconomic impacts can inform educational equity policies\n",
    "4. **Student Wellbeing:** Lifestyle factors help us understand the balance between academics and health\n",
    "5. **Predictive Modeling:** The variety of feature types (numeric, categorical, binary) allows us to practice different encoding and modeling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "In this section, we demonstrate our data cleaning process, including:\n",
    "- Loading and initial inspection\n",
    "- Missing value detection and analysis\n",
    "- Imputation strategies\n",
    "- Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/StudentPerformanceFactors.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numeric features\n",
    "print(\"Statistical Summary of Numeric Features:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Missing Value Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\"*50)\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data.to_string(index=False))\n",
    "    print(f\"\\nTotal columns with missing values: {len(missing_data)}\")\n",
    "else:\n",
    "    print(\"No missing values detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values pattern\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap (Yellow = Missing)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Features')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Bar plot of missing values\n",
    "    missing_counts = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "    if len(missing_counts) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        missing_counts.plot(kind='bar', color='coral')\n",
    "        plt.title('Missing Values Count by Feature', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Number of Missing Values')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No missing values to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Value Imputation Strategy\n",
    "\n",
    "Based on our analysis, we will apply appropriate imputation methods:\n",
    "\n",
    "**Imputation Principles:**\n",
    "1. **Categorical Variables:** Use mode (most frequent value) imputation\n",
    "2. **Numeric Variables:** Consider median for skewed distributions, mean for normal distributions\n",
    "3. **Missing Completely At Random (MCAR):** If missingness is < 5%, simple imputation is acceptable\n",
    "4. **Domain Knowledge:** Consider creating an \"Unknown\" category if missingness might be informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Identify columns with missing values\n",
    "columns_with_missing = df_cleaned.columns[df_cleaned.isnull().any()].tolist()\n",
    "\n",
    "print(\"Applying Imputation Strategies:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(columns_with_missing) > 0:\n",
    "    for col in columns_with_missing:\n",
    "        # Get data type\n",
    "        if df_cleaned[col].dtype == 'object':  # Categorical\n",
    "            mode_value = df_cleaned[col].mode()[0]\n",
    "            df_cleaned[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"✓ {col}: Filled {df[col].isnull().sum()} missing values with mode '{mode_value}'\")\n",
    "        else:  # Numeric\n",
    "            median_value = df_cleaned[col].median()\n",
    "            df_cleaned[col].fillna(median_value, inplace=True)\n",
    "            print(f\"✓ {col}: Filled {df[col].isnull().sum()} missing values with median {median_value}\")\n",
    "    \n",
    "    print(f\"\\nImputation complete! Remaining missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"No missing values to impute.\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "assert df_cleaned.isnull().sum().sum() == 0, \"Error: Missing values still present!\"\n",
    "print(\"\\n✓ Verification passed: All missing values have been handled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Outlier Detection\n",
    "\n",
    "We'll check for outliers in numeric features using the IQR (Interquartile Range) method and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric columns for outlier analysis: {numeric_cols}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].boxplot(df_cleaned[col].dropna(), vert=True)\n",
    "        axes[idx].set_title(f'{col}', fontweight='bold')\n",
    "        axes[idx].set_ylabel('Value')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide extra subplots if any\n",
    "for idx in range(len(numeric_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Box Plots for Outlier Detection', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR-based outlier detection\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"Outlier Detection (IQR Method):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<25} {'Outliers':<12} {'Lower Bound':<15} {'Upper Bound':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    n_outliers, lower, upper = detect_outliers_iqr(df_cleaned, col)\n",
    "    print(f\"{col:<25} {n_outliers:<12} {lower:<15.2f} {upper:<15.2f}\")\n",
    "\n",
    "print(\"\\nNote: We retain outliers as they may represent genuine data variation in educational contexts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Cleaning Summary\n",
    "\n",
    "**Actions Taken:**\n",
    "1. ✓ Loaded dataset (6,607 records, 20 features)\n",
    "2. ✓ Identified and visualized missing values\n",
    "3. ✓ Imputed missing categorical variables using mode\n",
    "4. ✓ Imputed missing numeric variables using median\n",
    "5. ✓ Detected outliers using IQR method and box plots\n",
    "6. ✓ Verified data quality\n",
    "\n",
    "**Dataset Status:** Ready for exploratory data analysis and modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we perform exploratory analysis to:\n",
    "1. Understand the distribution of our target variable (Exam_Score)\n",
    "2. Identify relationships between features and exam scores\n",
    "3. Determine the best predictor variables for simple linear regression\n",
    "4. Visualize patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable: Exam_Score\n",
    "print(\"Exam Score Distribution Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean: {df_cleaned['Exam_Score'].mean():.2f}\")\n",
    "print(f\"Median: {df_cleaned['Exam_Score'].median():.2f}\")\n",
    "print(f\"Std Dev: {df_cleaned['Exam_Score'].std():.2f}\")\n",
    "print(f\"Min: {df_cleaned['Exam_Score'].min():.2f}\")\n",
    "print(f\"Max: {df_cleaned['Exam_Score'].max():.2f}\")\n",
    "print(f\"Range: {df_cleaned['Exam_Score'].max() - df_cleaned['Exam_Score'].min():.2f}\")\n",
    "\n",
    "# Check for normality\n",
    "skewness = df_cleaned['Exam_Score'].skew()\n",
    "kurtosis = df_cleaned['Exam_Score'].kurtosis()\n",
    "print(f\"\\nSkewness: {skewness:.3f} {'(approximately symmetric)' if abs(skewness) < 0.5 else '(skewed)'}\")\n",
    "print(f\"Kurtosis: {kurtosis:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Exam_Score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram with KDE\n",
    "axes[0].hist(df_cleaned['Exam_Score'], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0].axvline(df_cleaned['Exam_Score'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_cleaned['Exam_Score'].mean():.1f}\")\n",
    "axes[0].axvline(df_cleaned['Exam_Score'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_cleaned['Exam_Score'].median():.1f}\")\n",
    "axes[0].set_xlabel('Exam Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Exam Scores', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df_cleaned['Exam_Score'], vert=True)\n",
    "axes[1].set_ylabel('Exam Score', fontsize=12)\n",
    "axes[1].set_title('Exam Score Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Correlation Analysis\n",
    "\n",
    "**Why correlation analysis?**\n",
    "- Identifies which numeric features have the strongest linear relationships with exam scores\n",
    "- Helps select the best predictor for simple linear regression\n",
    "- Reveals multicollinearity issues between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numeric features\n",
    "correlation_matrix = df_cleaned[numeric_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Numeric Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with Exam_Score (sorted)\n",
    "exam_score_corr = correlation_matrix['Exam_Score'].drop('Exam_Score').sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation with Exam Score (Ranked):\")\n",
    "print(\"=\"*50)\n",
    "for feature, corr in exam_score_corr.items():\n",
    "    print(f\"{feature:<25} {corr:>6.3f}\")\n",
    "\n",
    "# Visualize correlations with Exam_Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "exam_score_corr.plot(kind='barh', color=['green' if x > 0 else 'red' for x in exam_score_corr])\n",
    "plt.title('Feature Correlations with Exam Score', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Bivariate Analysis: Top Predictors vs Exam Score\n",
    "\n",
    "Based on correlation analysis, we'll create scatter plots for the top predictors to visualize their relationships with exam scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 6 correlated features\n",
    "top_features = exam_score_corr.abs().nlargest(6).index.tolist()\n",
    "\n",
    "print(f\"Top 6 Features Most Correlated with Exam Score:\")\n",
    "print(\"=\"*50)\n",
    "for i, feature in enumerate(top_features, 1):\n",
    "    print(f\"{i}. {feature}: {exam_score_corr[feature]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for top features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    axes[idx].scatter(df_cleaned[feature], df_cleaned['Exam_Score'], alpha=0.5, s=20)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(df_cleaned[feature], df_cleaned['Exam_Score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(df_cleaned[feature], p(df_cleaned[feature]), \"r--\", linewidth=2, label='Trend line')\n",
    "    \n",
    "    axes[idx].set_xlabel(feature, fontsize=11)\n",
    "    axes[idx].set_ylabel('Exam Score', fontsize=11)\n",
    "    axes[idx].set_title(f'{feature} vs Exam Score\\n(r = {exam_score_corr[feature]:.3f})', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.suptitle('Top Predictors vs Exam Score', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Categorical Features Analysis\n",
    "\n",
    "We'll analyze how categorical variables relate to exam scores using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key categorical features\n",
    "categorical_features = ['Parental_Involvement', 'Access_to_Resources', 'Motivation_Level', \n",
    "                        'Teacher_Quality', 'School_Type', 'Peer_Influence']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    if feature in df_cleaned.columns:\n",
    "        df_cleaned.boxplot(column='Exam_Score', by=feature, ax=axes[idx])\n",
    "        axes[idx].set_xlabel(feature, fontsize=11)\n",
    "        axes[idx].set_ylabel('Exam Score', fontsize=11)\n",
    "        axes[idx].set_title(f'Exam Score by {feature}', fontsize=12, fontweight='bold')\n",
    "        plt.sca(axes[idx])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('Exam Score Distribution by Categorical Features', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Preparing for Simple Linear Regression\n",
    "\n",
    "**Goal:** Identify the single best predictor for simple linear regression.\n",
    "\n",
    "**Selection Criteria:**\n",
    "1. Highest absolute correlation with Exam_Score\n",
    "2. Clear linear relationship visible in scatter plot\n",
    "3. No extreme outliers that would distort the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best single predictor\n",
    "best_predictor = exam_score_corr.abs().idxmax()\n",
    "best_correlation = exam_score_corr[best_predictor]\n",
    "\n",
    "print(\"Best Single Predictor for Simple Linear Regression:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Feature: {best_predictor}\")\n",
    "print(f\"Correlation: {best_correlation:.4f}\")\n",
    "print(f\"R-squared (r²): {best_correlation**2:.4f}\")\n",
    "print(f\"\\nInterpretation: {best_predictor} explains approximately {(best_correlation**2)*100:.1f}% \")\n",
    "print(f\"of the variance in Exam Scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed visualization of best predictor\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate statistics\n",
    "corr_coef, p_value = pearsonr(df_cleaned[best_predictor], df_cleaned['Exam_Score'])\n",
    "\n",
    "# Create detailed plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(df_cleaned[best_predictor], df_cleaned['Exam_Score'], alpha=0.5, s=30, label='Data points')\n",
    "\n",
    "# Fit regression line\n",
    "z = np.polyfit(df_cleaned[best_predictor], df_cleaned['Exam_Score'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df_cleaned[best_predictor], p(df_cleaned[best_predictor]), \"r-\", linewidth=3, \n",
    "         label=f'Regression line: y = {z[0]:.3f}x + {z[1]:.3f}')\n",
    "\n",
    "plt.xlabel(best_predictor, fontsize=13)\n",
    "plt.ylabel('Exam Score', fontsize=13)\n",
    "plt.title(f'Simple Linear Regression: {best_predictor} vs Exam Score\\n' +\n",
    "          f'Correlation: r = {corr_coef:.4f}, R² = {corr_coef**2:.4f}, p-value < 0.001', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStatistical Significance:\")\n",
    "print(f\"Pearson correlation coefficient: {corr_coef:.4f}\")\n",
    "print(f\"P-value: {p_value:.2e}\")\n",
    "print(f\"Result: {'Statistically significant' if p_value < 0.05 else 'Not significant'} at α = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Importance Summary for Linear Regression\n",
    "\n",
    "Based on our EDA, we can rank features by their potential predictive power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature ranking\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': exam_score_corr.index,\n",
    "    'Correlation': exam_score_corr.values,\n",
    "    'Abs_Correlation': exam_score_corr.abs().values,\n",
    "    'R_Squared': (exam_score_corr.values ** 2)\n",
    "}).sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(\"Feature Ranking for Predictive Modeling:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Rank':<6} {'Feature':<25} {'Correlation':<15} {'R²':<10}\")\n",
    "print(\"-\"*70)\n",
    "for idx, row in enumerate(feature_ranking.itertuples(), 1):\n",
    "    print(f\"{idx:<6} {row.Feature:<25} {row.Correlation:<15.4f} {row.R_Squared:<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Recommendations for Simple Linear Regression:\")\n",
    "print(f\"1. Primary predictor: {feature_ranking.iloc[0]['Feature']} (r = {feature_ranking.iloc[0]['Correlation']:.3f})\")\n",
    "print(f\"2. Alternative predictor: {feature_ranking.iloc[1]['Feature']} (r = {feature_ranking.iloc[1]['Correlation']:.3f})\")\n",
    "print(f\"3. Third option: {feature_ranking.iloc[2]['Feature']} (r = {feature_ranking.iloc[2]['Correlation']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **Dataset Selection** ✓\n",
    "   - Selected Student Performance Factors dataset (6,607 students, 20 variables)\n",
    "   - Identified research goal: predict exam scores from diverse factors\n",
    "\n",
    "2. **Feature Understanding** ✓\n",
    "   - Categorized 19 features into 5 domains (academic, family, resources, lifestyle, environmental)\n",
    "   - Explained rationale for studying each feature category\n",
    "\n",
    "3. **Data Cleaning** ✓\n",
    "   - Detected and visualized missing values\n",
    "   - Applied appropriate imputation (mode for categorical, median for numeric)\n",
    "   - Identified outliers using IQR method and box plots\n",
    "   - Verified data quality (zero missing values after cleaning)\n",
    "\n",
    "4. **Exploratory Data Analysis** ✓\n",
    "   - Analyzed target variable distribution (Exam_Score)\n",
    "   - Computed correlation matrix for all numeric features\n",
    "   - Identified top predictors through correlation analysis\n",
    "   - Visualized relationships using scatter plots and box plots\n",
    "   - Selected best predictor for simple linear regression\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- **Best predictor identified** for simple linear regression\n",
    "- **Strong correlations found** between academic factors and exam scores\n",
    "- **Data quality confirmed** - ready for modeling\n",
    "- **Relationships visualized** between features and target variable\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Build simple linear regression model using top predictor\n",
    "2. Evaluate model performance (MSE, RMSE, R²)\n",
    "3. Develop multiple linear regression with top features\n",
    "4. Encode categorical variables for advanced modeling\n",
    "5. Compare different regression algorithms (Ridge, Lasso, Random Forest, XGBoost)\n",
    "6. Perform feature engineering to create interaction terms\n",
    "7. Conduct cross-validation for robust model evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset is ready for machine learning modeling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
