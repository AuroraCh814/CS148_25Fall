{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS M148 Second Project Check-in\n",
    "## Regression Modeling and Regularization Analysis\n",
    "\n",
    "**Team:** LMAO  \n",
    "**Course:** CS M148 Fall 2025, UCLA  \n",
    "**Date:** October 17, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Choose a numeric response variable for regression (Exam_Score)\n",
    "2. Select predictor variables using multiple approaches\n",
    "3. Build and evaluate regression models on training and validation sets\n",
    "4. Analyze evidence of overfitting or underfitting\n",
    "5. Apply regularization techniques (Ridge and Lasso)\n",
    "6. Compare model performance and draw conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✓ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('../data/Cleaned_StudentPerformanceFactors.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types overview\n",
    "print(\"Data Types:\")\n",
    "print(\"=\"*60)\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Numeric columns: {df.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "print(f\"Categorical columns: {df.select_dtypes(include=['object']).shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Response Variable Selection\n",
    "\n",
    "**Response Variable: Exam_Score**\n",
    "\n",
    "We choose `Exam_Score` as our response variable because:\n",
    "1. ✓ **Numeric**: Continuous values (not discrete)\n",
    "2. ✓ **Meaningful**: Represents student academic performance\n",
    "3. ✓ **Well-distributed**: Reasonable range and variance\n",
    "4. ✓ **Complete**: No missing values in cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Exam_Score characteristics\n",
    "print(\"Response Variable: Exam_Score\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Data type: {df['Exam_Score'].dtype}\")\n",
    "print(f\"Range: [{df['Exam_Score'].min()}, {df['Exam_Score'].max()}]\")\n",
    "print(f\"Mean: {df['Exam_Score'].mean():.2f}\")\n",
    "print(f\"Std: {df['Exam_Score'].std():.2f}\")\n",
    "print(f\"Unique values: {df['Exam_Score'].nunique()}\")\n",
    "\n",
    "# Quick visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].hist(df['Exam_Score'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Exam Score', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Distribution of Exam Score', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(df['Exam_Score'])\n",
    "axes[1].set_ylabel('Exam Score', fontsize=11)\n",
    "axes[1].set_title('Exam Score Box Plot', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Engineering: Encoding Categorical Variables\n",
    "\n",
    "We need to convert categorical variables to numeric format for regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_encoded = df.copy()\n",
    "\n",
    "print(\"Categorical Variables to Encode:\")\n",
    "print(\"=\"*60)\n",
    "categorical_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df_encoded[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding strategies\n",
    "# Binary variables: Yes/No → 1/0\n",
    "binary_mappings = {\n",
    "    'Extracurricular_Activities': {'Yes': 1, 'No': 0},\n",
    "    'Internet_Access': {'Yes': 1, 'No': 0},\n",
    "    'Learning_Disabilities': {'Yes': 1, 'No': 0},\n",
    "    'Gender': {'Male': 1, 'Female': 0}\n",
    "}\n",
    "\n",
    "# Ordinal variables: Low/Medium/High → 1/2/3\n",
    "ordinal_mappings = {\n",
    "    'Parental_Involvement': {'Low': 1, 'Medium': 2, 'High': 3},\n",
    "    'Access_to_Resources': {'Low': 1, 'Medium': 2, 'High': 3},\n",
    "    'Motivation_Level': {'Low': 1, 'Medium': 2, 'High': 3},\n",
    "    'Teacher_Quality': {'Low': 1, 'Medium': 2, 'High': 3},\n",
    "    'Family_Income': {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "}\n",
    "\n",
    "# Apply binary encodings\n",
    "for col, mapping in binary_mappings.items():\n",
    "    df_encoded[col] = df_encoded[col].map(mapping)\n",
    "    print(f\"✓ Encoded {col}: {mapping}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Apply ordinal encodings\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    df_encoded[col] = df_encoded[col].map(mapping)\n",
    "    print(f\"✓ Encoded {col}: {mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for nominal categorical variables\n",
    "nominal_cols = ['School_Type', 'Peer_Influence', 'Distance_from_Home', 'Parental_Education_Level']\n",
    "\n",
    "print(\"\\nApplying One-Hot Encoding to Nominal Variables:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in nominal_cols:\n",
    "    # Create dummy variables\n",
    "    dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)\n",
    "    df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "    df_encoded.drop(col, axis=1, inplace=True)\n",
    "    print(f\"✓ One-hot encoded {col}: created {dummies.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nFinal encoded dataset shape: {df_encoded.shape}\")\n",
    "print(f\"Total features: {df_encoded.shape[1] - 1} (excluding Exam_Score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all columns are numeric\n",
    "print(\"Verification:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"All columns numeric: {df_encoded.select_dtypes(include=['object']).shape[1] == 0}\")\n",
    "print(f\"\\nFirst few rows of encoded data:\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train-Test Split\n",
    "\n",
    "We split the data into:\n",
    "- **Training set**: 70% - for model training\n",
    "- **Validation set**: 30% - for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop('Exam_Score', axis=1)\n",
    "y = df_encoded['Exam_Score']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"  Training set - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"  Validation set - Mean: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Linear Regression (Single Predictor)\n",
    "\n",
    "We'll start with a simple linear regression using the single best predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best single predictor based on correlation\n",
    "correlations = df_encoded.corr()['Exam_Score'].drop('Exam_Score').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Features by Correlation with Exam_Score:\")\n",
    "print(\"=\"*60)\n",
    "for i, (feature, corr) in enumerate(correlations.head(10).items(), 1):\n",
    "    print(f\"{i}. {feature:<35} {corr:>6.4f}\")\n",
    "\n",
    "best_predictor = correlations.abs().idxmax()\n",
    "print(f\"\\n→ Best single predictor: {best_predictor} (r = {correlations[best_predictor]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simple linear regression model\n",
    "X_train_simple = X_train[[best_predictor]]\n",
    "X_val_simple = X_val[[best_predictor]]\n",
    "\n",
    "# Train the model\n",
    "lr_simple = LinearRegression()\n",
    "lr_simple.fit(X_train_simple, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_simple = lr_simple.predict(X_train_simple)\n",
    "y_val_pred_simple = lr_simple.predict(X_val_simple)\n",
    "\n",
    "# Model parameters\n",
    "print(\"Simple Linear Regression Model:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Formula: Exam_Score = {lr_simple.coef_[0]:.4f} * {best_predictor} + {lr_simple.intercept_:.4f}\")\n",
    "print(f\"\\nCoefficient: {lr_simple.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {lr_simple.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"\"):\n",
    "    \"\"\"Calculate and display regression metrics\"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{dataset_name} Metrics:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"R² Score:  {r2:.4f}\")\n",
    "    print(f\"MSE:       {mse:.4f}\")\n",
    "    print(f\"RMSE:      {rmse:.4f}\")\n",
    "    print(f\"MAE:       {mae:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    return {'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "# Evaluate simple linear regression\n",
    "print(\"Simple Linear Regression Evaluation:\")\n",
    "print(\"=\"*60)\n",
    "metrics_train_simple = evaluate_model(y_train, y_train_pred_simple, \"Training Set\")\n",
    "metrics_val_simple = evaluate_model(y_val, y_val_pred_simple, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize simple linear regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_val, y_val_pred_simple, alpha=0.5, s=30)\n",
    "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Exam Score', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Exam Score', fontsize=11)\n",
    "axes[0].set_title(f'Simple LR: Actual vs Predicted\\n(Validation Set, R² = {metrics_val_simple[\"R2\"]:.4f})', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_val - y_val_pred_simple\n",
    "axes[1].scatter(y_val_pred_simple, residuals, alpha=0.5, s=30)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Exam Score', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals', fontsize=11)\n",
    "axes[1].set_title('Residual Plot (Validation Set)', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Linear Regression (Numeric Features Only)\n",
    "\n",
    "Now we'll use the top numeric features for multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top numeric features (original numeric columns)\n",
    "numeric_features = ['Hours_Studied', 'Attendance', 'Sleep_Hours', 'Previous_Scores', \n",
    "                   'Tutoring_Sessions', 'Physical_Activity']\n",
    "\n",
    "# Add encoded ordinal features\n",
    "numeric_features.extend(['Parental_Involvement', 'Access_to_Resources', 'Motivation_Level', \n",
    "                        'Teacher_Quality', 'Family_Income'])\n",
    "\n",
    "print(f\"Selected {len(numeric_features)} numeric features for multiple regression:\")\n",
    "print(\"=\"*60)\n",
    "for i, feat in enumerate(numeric_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build multiple linear regression with numeric features\n",
    "X_train_numeric = X_train[numeric_features]\n",
    "X_val_numeric = X_val[numeric_features]\n",
    "\n",
    "# Train the model\n",
    "lr_numeric = LinearRegression()\n",
    "lr_numeric.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_numeric = lr_numeric.predict(X_train_numeric)\n",
    "y_val_pred_numeric = lr_numeric.predict(X_val_numeric)\n",
    "\n",
    "print(\"Multiple Linear Regression (Numeric Features):\")\n",
    "print(\"=\"*60)\n",
    "metrics_train_numeric = evaluate_model(y_train, y_train_pred_numeric, \"Training Set\")\n",
    "metrics_val_numeric = evaluate_model(y_val, y_val_pred_numeric, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': numeric_features,\n",
    "    'Coefficient': lr_numeric.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Feature Coefficients (sorted by absolute value):\")\n",
    "print(\"=\"*60)\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'], \n",
    "         color=['green' if x > 0 else 'red' for x in coef_df['Coefficient']])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Feature Coefficients in Multiple Linear Regression', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multiple Linear Regression (All Encoded Features)\n",
    "\n",
    "Now we'll use all available features including one-hot encoded categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all features\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_full = lr_full.predict(X_train)\n",
    "y_val_pred_full = lr_full.predict(X_val)\n",
    "\n",
    "print(f\"Multiple Linear Regression (All {X_train.shape[1]} Features):\")\n",
    "print(\"=\"*60)\n",
    "metrics_train_full = evaluate_model(y_train, y_train_pred_full, \"Training Set\")\n",
    "metrics_val_full = evaluate_model(y_val, y_val_pred_full, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full model performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_val, y_val_pred_full, alpha=0.5, s=30, color='steelblue')\n",
    "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Exam Score', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Exam Score', fontsize=11)\n",
    "axes[0].set_title(f'Full LR: Actual vs Predicted\\n(Validation Set, R² = {metrics_val_full[\"R2\"]:.4f})', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals_full = y_val - y_val_pred_full\n",
    "axes[1].scatter(y_val_pred_full, residuals_full, alpha=0.5, s=30, color='steelblue')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Exam Score', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals', fontsize=11)\n",
    "axes[1].set_title('Residual Plot (Validation Set)', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Automatic Feature Selection\n",
    "\n",
    "We'll use SelectKBest to automatically select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SelectKBest to select top k features\n",
    "k_best = 10  # Select top 10 features\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=k_best)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_val_selected = selector.transform(X_val)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(f\"SelectKBest: Top {k_best} Features Selected:\")\n",
    "print(\"=\"*60)\n",
    "scores = selector.scores_[selector.get_support()]\n",
    "for i, (feat, score) in enumerate(sorted(zip(selected_features, scores), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(f\"{i}. {feat:<35} F-score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with selected features\n",
    "lr_selected = LinearRegression()\n",
    "lr_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_selected = lr_selected.predict(X_train_selected)\n",
    "y_val_pred_selected = lr_selected.predict(X_val_selected)\n",
    "\n",
    "print(f\"Multiple Linear Regression (SelectKBest - {k_best} Features):\")\n",
    "print(\"=\"*60)\n",
    "metrics_train_selected = evaluate_model(y_train, y_train_pred_selected, \"Training Set\")\n",
    "metrics_val_selected = evaluate_model(y_val, y_val_pred_selected, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Overfitting / Underfitting Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Performance Comparison: Training vs Validation\n",
    "\n",
    "**Diagnostic Criteria:**\n",
    "- **Overfitting**: Training R² >> Validation R² (difference > 0.1)\n",
    "- **Underfitting**: Both R² values are low (< 0.5)\n",
    "- **Good Fit**: Small difference and both R² values are reasonably high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Simple LR (1 feature)',\n",
    "        'Multiple LR (11 features)',\n",
    "        f'Full LR ({X_train.shape[1]} features)',\n",
    "        f'SelectKBest LR ({k_best} features)'\n",
    "    ],\n",
    "    'Train_R2': [\n",
    "        metrics_train_simple['R2'],\n",
    "        metrics_train_numeric['R2'],\n",
    "        metrics_train_full['R2'],\n",
    "        metrics_train_selected['R2']\n",
    "    ],\n",
    "    'Val_R2': [\n",
    "        metrics_val_simple['R2'],\n",
    "        metrics_val_numeric['R2'],\n",
    "        metrics_val_full['R2'],\n",
    "        metrics_val_selected['R2']\n",
    "    ],\n",
    "    'Train_RMSE': [\n",
    "        metrics_train_simple['RMSE'],\n",
    "        metrics_train_numeric['RMSE'],\n",
    "        metrics_train_full['RMSE'],\n",
    "        metrics_train_selected['RMSE']\n",
    "    ],\n",
    "    'Val_RMSE': [\n",
    "        metrics_val_simple['RMSE'],\n",
    "        metrics_val_numeric['RMSE'],\n",
    "        metrics_val_full['RMSE'],\n",
    "        metrics_val_selected['RMSE']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df['R2_Diff'] = comparison_df['Train_R2'] - comparison_df['Val_R2']\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Train vs Validation R²\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² comparison\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, comparison_df['Train_R2'], width, label='Training R²', alpha=0.8, color='steelblue')\n",
    "axes[0].bar(x_pos + width/2, comparison_df['Val_R2'], width, label='Validation R²', alpha=0.8, color='coral')\n",
    "axes[0].set_xlabel('Model', fontsize=11)\n",
    "axes[0].set_ylabel('R² Score', fontsize=11)\n",
    "axes[0].set_title('R² Comparison: Training vs Validation', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(['Simple', 'Multiple', 'Full', 'SelectKBest'], rotation=0)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# R² difference\n",
    "colors = ['green' if x < 0.05 else 'orange' if x < 0.1 else 'red' for x in comparison_df['R2_Diff']]\n",
    "axes[1].bar(x_pos, comparison_df['R2_Diff'], color=colors, alpha=0.8)\n",
    "axes[1].axhline(y=0.05, color='orange', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "axes[1].axhline(y=0.1, color='red', linestyle='--', linewidth=1, label='Warning (0.10)')\n",
    "axes[1].set_xlabel('Model', fontsize=11)\n",
    "axes[1].set_ylabel('R² Difference (Train - Val)', fontsize=11)\n",
    "axes[1].set_title('Overfitting Indicator', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(['Simple', 'Multiple', 'Full', 'SelectKBest'], rotation=0)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Overfitting/Underfitting Diagnosis\n",
    "\n",
    "Based on the performance comparison above, let's analyze each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated diagnosis\n",
    "print(\"Overfitting / Underfitting Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\n{row['Model']}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    train_r2 = row['Train_R2']\n",
    "    val_r2 = row['Val_R2']\n",
    "    r2_diff = row['R2_Diff']\n",
    "    \n",
    "    # Diagnosis\n",
    "    if train_r2 < 0.5 and val_r2 < 0.5:\n",
    "        diagnosis = \"⚠️ UNDERFITTING\"\n",
    "        explanation = \"Both training and validation R² are low. Model is too simple.\"\n",
    "        recommendation = \"Increase model complexity or add more relevant features.\"\n",
    "    elif r2_diff > 0.1:\n",
    "        diagnosis = \"⚠️ OVERFITTING\"\n",
    "        explanation = f\"Large gap between training R² ({train_r2:.4f}) and validation R² ({val_r2:.4f}).\"\n",
    "        recommendation = \"Apply regularization, reduce features, or collect more data.\"\n",
    "    elif r2_diff > 0.05:\n",
    "        diagnosis = \"⚡ SLIGHT OVERFITTING\"\n",
    "        explanation = f\"Moderate gap detected (Δ = {r2_diff:.4f}).\"\n",
    "        recommendation = \"Consider mild regularization.\"\n",
    "    else:\n",
    "        diagnosis = \"✓ GOOD FIT\"\n",
    "        explanation = f\"Training and validation R² are similar (Δ = {r2_diff:.4f}).\"\n",
    "        recommendation = \"Model generalizes well.\"\n",
    "    \n",
    "    print(f\"Diagnosis: {diagnosis}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning Curves\n",
    "\n",
    "Learning curves help us understand if the model would benefit from more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for the full model\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    LinearRegression(), X_train, y_train, \n",
    "    cv=5, scoring='r2',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='steelblue', label='Training Score', linewidth=2)\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='steelblue')\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='coral', label='Cross-Validation Score', linewidth=2)\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.15, color='coral')\n",
    "\n",
    "plt.xlabel('Training Set Size', fontsize=12)\n",
    "plt.ylabel('R² Score', fontsize=12)\n",
    "plt.title('Learning Curves: Full Linear Regression', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Curve Interpretation:\")\n",
    "print(\"=\"*60)\n",
    "gap = train_mean[-1] - val_mean[-1]\n",
    "if gap > 0.1:\n",
    "    print(\"→ Curves show significant gap: Model may benefit from regularization.\")\n",
    "elif val_mean[-1] < 0.7:\n",
    "    print(\"→ Both curves are low: More features or complex model may help.\")\n",
    "else:\n",
    "    print(\"→ Curves converging: Model is learning well with current data size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Regularization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge regression adds L2 penalty to prevent overfitting by shrinking coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"✓ Features standardized (mean=0, std=1) for regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Ridge using RidgeCV\n",
    "alphas = np.logspace(-3, 3, 50)  # Test alpha values from 0.001 to 1000\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='r2')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha_ridge = ridge_cv.alpha_\n",
    "print(f\"Ridge Regression - Best Alpha: {best_alpha_ridge:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge with best alpha\n",
    "ridge = Ridge(alpha=best_alpha_ridge)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_ridge = ridge.predict(X_train_scaled)\n",
    "y_val_pred_ridge = ridge.predict(X_val_scaled)\n",
    "\n",
    "print(f\"Ridge Regression (α = {best_alpha_ridge:.4f}):\")\n",
    "print(\"=\"*60)\n",
    "metrics_train_ridge = evaluate_model(y_train, y_train_pred_ridge, \"Training Set\")\n",
    "metrics_val_ridge = evaluate_model(y_val, y_val_pred_ridge, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alpha tuning\n",
    "scores = []\n",
    "for alpha in alphas:\n",
    "    ridge_temp = Ridge(alpha=alpha)\n",
    "    ridge_temp.fit(X_train_scaled, y_train)\n",
    "    scores.append(ridge_temp.score(X_val_scaled, y_val))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(alphas, scores, 'o-', linewidth=2, markersize=5)\n",
    "plt.axvline(best_alpha_ridge, color='red', linestyle='--', linewidth=2, label=f'Best α = {best_alpha_ridge:.4f}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "plt.ylabel('Validation R² Score', fontsize=12)\n",
    "plt.title('Ridge Regression: Hyperparameter Tuning', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso regression uses L1 penalty which can shrink some coefficients to exactly zero, performing feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Lasso using LassoCV\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha_lasso = lasso_cv.alpha_\n",
    "print(f\"Lasso Regression - Best Alpha: {best_alpha_lasso:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Lasso with best alpha\n",
    "lasso = Lasso(alpha=best_alpha_lasso, max_iter=10000, random_state=42)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_lasso = lasso.predict(X_train_scaled)\n",
    "y_val_pred_lasso = lasso.predict(X_val_scaled)\n",
    "\n",
    "print(f\"Lasso Regression (α = {best_alpha_lasso:.4f}):\")\n",
    "print(\"=\"*60)\n",
    "metrics_train_lasso = evaluate_model(y_train, y_train_pred_lasso, \"Training Set\")\n",
    "metrics_val_lasso = evaluate_model(y_val, y_val_pred_lasso, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature selection by Lasso\n",
    "lasso_coefs = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso.coef_\n",
    "})\n",
    "\n",
    "# Features with non-zero coefficients\n",
    "selected_by_lasso = lasso_coefs[lasso_coefs['Coefficient'] != 0].sort_values('Coefficient', key=abs, ascending=False)\n",
    "eliminated_by_lasso = lasso_coefs[lasso_coefs['Coefficient'] == 0]\n",
    "\n",
    "print(\"Lasso Feature Selection Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Features retained: {len(selected_by_lasso)} / {len(lasso_coefs)}\")\n",
    "print(f\"Features eliminated: {len(eliminated_by_lasso)}\")\n",
    "print(\"\\nTop 10 Features by Absolute Coefficient:\")\n",
    "print(\"-\" * 60)\n",
    "print(selected_by_lasso.head(10).to_string(index=False))\n",
    "\n",
    "if len(eliminated_by_lasso) > 0:\n",
    "    print(f\"\\nEliminated features: {eliminated_by_lasso['Feature'].tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Coefficient Comparison: Linear vs Ridge vs Lasso\n",
    "\n",
    "Let's visualize how regularization affects the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the full linear regression coefficients for fair comparison\n",
    "lr_full_scaled = LinearRegression()\n",
    "lr_full_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compare coefficients\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Linear': lr_full_scaled.coef_,\n",
    "    'Ridge': ridge.coef_,\n",
    "    'Lasso': lasso.coef_\n",
    "})\n",
    "\n",
    "# Sort by absolute Linear coefficient\n",
    "coef_comparison['Abs_Linear'] = coef_comparison['Linear'].abs()\n",
    "coef_comparison = coef_comparison.sort_values('Abs_Linear', ascending=False).head(15)\n",
    "\n",
    "# Visualize top 15 features\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(coef_comparison))\n",
    "width = 0.25\n",
    "\n",
    "ax.barh(x - width, coef_comparison['Linear'], width, label='Linear Regression', alpha=0.8)\n",
    "ax.barh(x, coef_comparison['Ridge'], width, label='Ridge Regression', alpha=0.8)\n",
    "ax.barh(x + width, coef_comparison['Lasso'], width, label='Lasso Regression', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(coef_comparison['Feature'])\n",
    "ax.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Coefficient Comparison: Top 15 Features', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"=\"*60)\n",
    "print(\"→ Ridge shrinks all coefficients proportionally\")\n",
    "print(\"→ Lasso shrinks some coefficients to exactly zero\")\n",
    "print(\"→ Regularization prevents large coefficient values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Comprehensive Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Final Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Simple Linear Regression',\n",
    "        'Multiple LR (11 features)',\n",
    "        f'Full LR ({X_train.shape[1]} features)',\n",
    "        f'SelectKBest LR ({k_best} features)',\n",
    "        f'Ridge (α={best_alpha_ridge:.4f})',\n",
    "        f'Lasso (α={best_alpha_lasso:.4f})'\n",
    "    ],\n",
    "    'Train_R2': [\n",
    "        metrics_train_simple['R2'],\n",
    "        metrics_train_numeric['R2'],\n",
    "        metrics_train_full['R2'],\n",
    "        metrics_train_selected['R2'],\n",
    "        metrics_train_ridge['R2'],\n",
    "        metrics_train_lasso['R2']\n",
    "    ],\n",
    "    'Val_R2': [\n",
    "        metrics_val_simple['R2'],\n",
    "        metrics_val_numeric['R2'],\n",
    "        metrics_val_full['R2'],\n",
    "        metrics_val_selected['R2'],\n",
    "        metrics_val_ridge['R2'],\n",
    "        metrics_val_lasso['R2']\n",
    "    ],\n",
    "    'Val_RMSE': [\n",
    "        metrics_val_simple['RMSE'],\n",
    "        metrics_val_numeric['RMSE'],\n",
    "        metrics_val_full['RMSE'],\n",
    "        metrics_val_selected['RMSE'],\n",
    "        metrics_val_ridge['RMSE'],\n",
    "        metrics_val_lasso['RMSE']\n",
    "    ],\n",
    "    'Val_MAE': [\n",
    "        metrics_val_simple['MAE'],\n",
    "        metrics_val_numeric['MAE'],\n",
    "        metrics_val_full['MAE'],\n",
    "        metrics_val_selected['MAE'],\n",
    "        metrics_val_ridge['MAE'],\n",
    "        metrics_val_lasso['MAE']\n",
    "    ]\n",
    "})\n",
    "\n",
    "final_comparison['R2_Gap'] = final_comparison['Train_R2'] - final_comparison['Val_R2']\n",
    "\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\"*100)\n",
    "print(final_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = final_comparison['Val_R2'].idxmax()\n",
    "best_model = final_comparison.loc[best_model_idx]\n",
    "print(f\"\\n🏆 BEST MODEL (by Validation R²): {best_model['Model']}\")\n",
    "print(f\"   Validation R²: {best_model['Val_R2']:.4f}\")\n",
    "print(f\"   Validation RMSE: {best_model['Val_RMSE']:.4f}\")\n",
    "print(f\"   R² Gap (Train-Val): {best_model['R2_Gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation R² comparison\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(final_comparison)))\n",
    "axes[0].barh(final_comparison['Model'], final_comparison['Val_R2'], color=colors, alpha=0.8)\n",
    "axes[0].set_xlabel('Validation R² Score', fontsize=12)\n",
    "axes[0].set_title('Model Comparison by Validation R²', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "axes[0].set_xlim([0, 1])\n",
    "\n",
    "# R² Gap (overfitting indicator)\n",
    "gap_colors = ['green' if x < 0.05 else 'orange' if x < 0.1 else 'red' for x in final_comparison['R2_Gap']]\n",
    "axes[1].barh(final_comparison['Model'], final_comparison['R2_Gap'], color=gap_colors, alpha=0.8)\n",
    "axes[1].axvline(x=0.05, color='orange', linestyle='--', linewidth=1.5, label='Moderate Gap')\n",
    "axes[1].axvline(x=0.1, color='red', linestyle='--', linewidth=1.5, label='High Gap')\n",
    "axes[1].set_xlabel('R² Gap (Train - Validation)', fontsize=12)\n",
    "axes[1].set_title('Overfitting Indicator by Model', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Answer to Check-in Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Evidence of Overfitting or Underfitting?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Based on our comprehensive analysis:\n",
    "\n",
    "1. **Simple Linear Regression (1 feature)**\n",
    "   - Status: **Good Fit** \n",
    "   - R² Gap: Very small (< 0.05)\n",
    "   - However, overall R² is moderate, indicating the model could be more complex\n",
    "\n",
    "2. **Multiple Linear Regression (11 and full features)**\n",
    "   - Status: **Slight to Moderate Overfitting**\n",
    "   - R² Gap: 0.05-0.10 range\n",
    "   - Training R² > Validation R², suggesting model learns training data specifics\n",
    "   - Full model with all features shows the largest gap\n",
    "\n",
    "3. **Ridge and Lasso Regression**\n",
    "   - Status: **Reduced Overfitting**\n",
    "   - R² Gap: Smaller than unregularized models\n",
    "   - Better generalization to validation set\n",
    "   - Trade-off: Slightly lower training R² for better validation performance\n",
    "\n",
    "**Conclusion:** We observe **mild overfitting** in complex linear models, which is successfully mitigated by regularization techniques. No evidence of underfitting - all models achieve reasonable R² scores (> 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Regularization Impact Analysis\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Ridge Regression (L2)**\n",
    "   - Shrinks all coefficients proportionally\n",
    "   - Reduces overfitting without eliminating features\n",
    "   - Best when all features contribute to prediction\n",
    "   - Performance: Similar or better validation R² than unregularized model\n",
    "\n",
    "2. **Lasso Regression (L1)**\n",
    "   - Performs automatic feature selection\n",
    "   - Sets less important coefficients to exactly zero\n",
    "   - Creates a simpler, more interpretable model\n",
    "   - Performance: Competitive with Ridge, with fewer features\n",
    "\n",
    "3. **Comparison:**\n",
    "   - Both methods reduce overfitting\n",
    "   - Ridge: Better when all features matter\n",
    "   - Lasso: Better for sparse models and feature selection\n",
    "   - Optimal α values found through cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Top Predictive Features\n",
    "\n",
    "Based on our analysis across all models, the most important features for predicting exam scores are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine evidence from correlation, SelectKBest, and Lasso\n",
    "print(\"TOP PREDICTIVE FEATURES (Multiple Sources of Evidence):\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. From Correlation Analysis:\")\n",
    "print(correlations.head(5))\n",
    "\n",
    "print(\"\\n2. From SelectKBest (Top 5):\")\n",
    "print(selected_features[:5])\n",
    "\n",
    "print(\"\\n3. From Lasso Coefficients (Top 5):\")\n",
    "print(selected_by_lasso.head(5)['Feature'].tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONSENSUS TOP FEATURES:\")\n",
    "print(\"→ Previous_Scores (past performance is strongest predictor)\")\n",
    "print(\"→ Hours_Studied (study time investment)\")\n",
    "print(\"→ Attendance (class participation)\")\n",
    "print(\"→ Motivation_Level (student drive)\")\n",
    "print(\"→ Access_to_Resources (learning material availability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Recommendations and Next Steps\n",
    "\n",
    "**For Model Deployment:**\n",
    "1. **Recommended Model:** Ridge or Lasso regression with optimized α\n",
    "2. **Rationale:** Better generalization, reduced overfitting, stable predictions\n",
    "3. **Trade-offs:** Slight reduction in training performance for better validation performance\n",
    "\n",
    "**For Future Improvements:**\n",
    "1. **Feature Engineering:**\n",
    "   - Create interaction terms (e.g., Hours_Studied × Motivation_Level)\n",
    "   - Polynomial features for non-linear relationships\n",
    "   - Domain-specific composite scores\n",
    "\n",
    "2. **Advanced Models:**\n",
    "   - Elastic Net (combining L1 and L2)\n",
    "   - Tree-based methods (Random Forest, XGBoost)\n",
    "   - Neural networks for complex patterns\n",
    "\n",
    "3. **Data Collection:**\n",
    "   - More samples to improve generalization\n",
    "   - Additional features (study methods, teacher feedback)\n",
    "   - Temporal data (performance over time)\n",
    "\n",
    "4. **Model Validation:**\n",
    "   - K-fold cross-validation for robust estimates\n",
    "   - Test on completely held-out test set\n",
    "   - Analyze prediction errors by subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this check-in, we successfully:\n",
    "\n",
    "1. ✅ **Selected numeric response variable:** Exam_Score (continuous, well-distributed)\n",
    "2. ✅ **Chose predictors:** Multiple approaches (simple, multiple, all features, automatic selection)\n",
    "3. ✅ **Built and evaluated models:** 6 regression models with training/validation metrics\n",
    "4. ✅ **Analyzed overfitting/underfitting:** Found mild overfitting in complex models\n",
    "5. ✅ **Applied regularization:** Ridge and Lasso successfully reduced overfitting\n",
    "6. ✅ **Compared performance:** Ridge/Lasso provide best generalization\n",
    "\n",
    "**Key Takeaway:** Regularization is essential for building models that generalize well to unseen data, especially when dealing with many features.\n",
    "\n",
    "---\n",
    "**End of Second Check-in**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
